# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "feb712bf-1546-445c-bea4-665c82c12f2b",
# META       "default_lakehouse_name": "pbisummitlake",
# META       "default_lakehouse_workspace_id": "4a689fc0-cc6e-482f-b6cf-3026a575a7c1"
# META     },
# META     "environment": {
# META       "environmentId": "28dbaf6b-8fc5-89a4-444a-db3b58a22f66",
# META       "workspaceId": "00000000-0000-0000-0000-000000000000"
# META     }
# META   }
# META }

# MARKDOWN ********************

# # Load Data
# 
# We are going to load information needed into a Delta Table from csv files. The code assumes that you did load the dunnhumby Carbon sample data into a folder called dunnhumby. You should change the path to the right one in your case
# 
# Code included here is for learning step by step. It is not ready to put into production since it could be better written to be more efficient

# CELL ********************

# You need to create a lakehouse and upload de .csv files from dunnhumby

df_transactions = spark.read.csv('abfss://4a689fc0-cc6e-482f-b6cf-3026a575a7c1@onelake.dfs.fabric.microsoft.com/feb712bf-1546-445c-bea4-665c82c12f2b/Files/dunnhumby/transaction_data.csv', header=True )


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Code generated by Data Wrangler for PySpark DataFrame

from pyspark.sql import functions as F
from pyspark.sql import types as T

def clean_data(df_transactions):
    # Change column type to int64 for columns: 'household_key', 'BASKET_ID' and 5 other columns
    df_transactions = df_transactions.withColumn('household_key', df_transactions['household_key'].cast(T.LongType()))
    df_transactions = df_transactions.withColumn('BASKET_ID', df_transactions['BASKET_ID'].cast(T.LongType()))
    df_transactions = df_transactions.withColumn('DAY', df_transactions['DAY'].cast(T.LongType()))
    df_transactions = df_transactions.withColumn('QUANTITY', df_transactions['QUANTITY'].cast(T.LongType()))
    df_transactions = df_transactions.withColumn('STORE_ID', df_transactions['STORE_ID'].cast(T.LongType()))
    df_transactions = df_transactions.withColumn('TRANS_TIME', df_transactions['TRANS_TIME'].cast(T.LongType()))
    df_transactions = df_transactions.withColumn('WEEK_NO', df_transactions['WEEK_NO'].cast(T.LongType()))
    # Change column type to float64 for columns: 'SALES_VALUE', 'RETAIL_DISC' and 2 other columns
    df_transactions = df_transactions.withColumn('SALES_VALUE', df_transactions['SALES_VALUE'].cast(T.DoubleType()))
    df_transactions = df_transactions.withColumn('RETAIL_DISC', df_transactions['RETAIL_DISC'].cast(T.DoubleType()))
    df_transactions = df_transactions.withColumn('COUPON_MATCH_DISC', df_transactions['COUPON_MATCH_DISC'].cast(T.DoubleType()))
    df_transactions = df_transactions.withColumn('COUPON_DISC', df_transactions['COUPON_DISC'].cast(T.DoubleType()))
    # Performed 3 aggregations grouped on column: 'household_key'
    df_transactions = df_transactions.groupBy('household_key').agg(F.countDistinct('BASKET_ID').alias('Frequency'), F.max('DAY').alias('Recency'), F.sum('SALES_VALUE').alias('MonetaryValue'))
    df_transactions = df_transactions.dropna()
    df_transactions = df_transactions.sort(df_transactions['household_key'].asc())
    return df_transactions

df_transactions_clean = clean_data(df_transactions)
display(df_transactions_clean)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# We are going to calculate a RFM (Recency, Frequency, Monetary) score for our customers and define lost customers as those with recency below 600. (They did their last transaction more than 600 days ago)

# CELL ********************

df_churn = df_transactions_clean.withColumn('churn',df_transactions_clean['Recency']>600)


display(df_churn)


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Code generated by Data Wrangler for PySpark DataFrame

from pyspark.sql import types as T

def clean_data(df_churn):
    # Change column type to int8 for column: 'churn'
    df_churn = df_churn.withColumn('churn', df_churn['churn'].cast(T.ByteType()))
    return df_churn

df_churn_clean = clean_data(df_churn)
display(df_churn_clean)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

join_customers = df_churn_clean.join(df_customers , on='household_key' , how='left')
display(join_customers)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

df_customers = spark.read.csv('abfss://4a689fc0-cc6e-482f-b6cf-3026a575a7c1@onelake.dfs.fabric.microsoft.com/feb712bf-1546-445c-bea4-665c82c12f2b/Files/dunnhumby/hh_demographic.csv',header=True)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

#create the delta table
join_customers.write.mode("overwrite").format("delta").saveAsTable("customertransactions")


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
