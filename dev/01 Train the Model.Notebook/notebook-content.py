# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "feb712bf-1546-445c-bea4-665c82c12f2b",
# META       "default_lakehouse_name": "pbisummitlake",
# META       "default_lakehouse_workspace_id": "4a689fc0-cc6e-482f-b6cf-3026a575a7c1"
# META     },
# META     "environment": {
# META       "environmentId": "28dbaf6b-8fc5-89a4-444a-db3b58a22f66",
# META       "workspaceId": "00000000-0000-0000-0000-000000000000"
# META     }
# META   }
# META }

# CELL ********************

df = spark.sql("SELECT * FROM pbisummitlake.customertransactions")
display(df)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************


features =['household_key','AGE_DESC' , 'MARITAL_STATUS_CODE' , 'INCOME_DESC' , 'HOMEOWNER_DESC' , 'HH_COMP_DESC' , 'HOUSEHOLD_SIZE_DESC' , 'KID_CATEGORY_DESC']
label = ['churn']

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Code generated by Data Wrangler for PySpark DataFrame

# As far as we are using functions such as OneHotEncoder this create an experiment with the same name as the Notebook automatically

from pyspark.ml.feature import StringIndexer, OneHotEncoder
from pyspark.ml.functions import vector_to_array
from pyspark.sql import functions as F

def clean_data(df):
    # One-hot encode columns: 'AGE_DESC', 'MARITAL_STATUS_CODE' and 5 other columns
    # ⚠️ This was generated to match the original pandas logic but may have performance issues.
    def one_hot_encode_col(df, key):
        insert_idx = df.columns.index(key)
        indexer = StringIndexer(inputCol=key, outputCol='%s_numeric' % str(key), handleInvalid='keep')
        indexer_fitted = indexer.fit(df)
        df_indexed = indexer_fitted.transform(df)
        encoder = OneHotEncoder(inputCols=['%s_numeric' % str(key)], outputCols=['%s_onehot' % str(key)], dropLast=False)
        df_onehot = encoder.fit(df_indexed).transform(df_indexed)
        df_col_onehot = df_onehot.select('*', vector_to_array('%s_onehot' % str(key)).alias('%s_col_onehot' % str(key)))
        labels = sorted(indexer_fitted.labels) + ['%s_nan' % str(key)]
        cols_expanded = [(F.col('%s_col_onehot' % str(key))[i].alias('%s_%s' % (str(key), labels[i]))) for i in range(len(labels))]
        df = df_col_onehot.select(*df.columns[:insert_idx], *cols_expanded, *df.columns[insert_idx+1:])
        return df
    df = one_hot_encode_col(df, 'AGE_DESC')
    df = one_hot_encode_col(df, 'MARITAL_STATUS_CODE')
    df = one_hot_encode_col(df, 'INCOME_DESC')
    df = one_hot_encode_col(df, 'HOMEOWNER_DESC')
    df = one_hot_encode_col(df, 'HH_COMP_DESC')
    df = one_hot_encode_col(df, 'HOUSEHOLD_SIZE_DESC')
    df = one_hot_encode_col(df, 'KID_CATEGORY_DESC')
    return df

df_clean = clean_data(df)
display(df_clean)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

feature_cols = ['household_key',
 'Frequency',
 'Recency',
 'MonetaryValue', 
 'AGE_DESC_19-24',
 'AGE_DESC_25-34',
 'AGE_DESC_35-44',
 'AGE_DESC_45-54',
 'AGE_DESC_55-64',
 'AGE_DESC_65+',
 'AGE_DESC_AGE_DESC_nan',
 'MARITAL_STATUS_CODE_A',
 'MARITAL_STATUS_CODE_B',
 'MARITAL_STATUS_CODE_U',
 'MARITAL_STATUS_CODE_MARITAL_STATUS_CODE_nan',
 'INCOME_DESC_100-124K',
 'INCOME_DESC_125-149K',
 'INCOME_DESC_15-24K',
 'INCOME_DESC_150-174K',
 'INCOME_DESC_175-199K',
 'INCOME_DESC_200-249K',
 'INCOME_DESC_25-34K',
 'INCOME_DESC_250K+',
 'INCOME_DESC_35-49K',
 'INCOME_DESC_50-74K',
 'INCOME_DESC_75-99K',
 'INCOME_DESC_Under 15K',
 'INCOME_DESC_INCOME_DESC_nan',
 'HOMEOWNER_DESC_Homeowner',
 'HOMEOWNER_DESC_Probable Owner',
 'HOMEOWNER_DESC_Probable Renter',
 'HOMEOWNER_DESC_Renter',
 'HOMEOWNER_DESC_Unknown',
 'HOMEOWNER_DESC_HOMEOWNER_DESC_nan',
 'HH_COMP_DESC_1 Adult Kids',
 'HH_COMP_DESC_2 Adults Kids',
 'HH_COMP_DESC_2 Adults No Kids',
 'HH_COMP_DESC_Single Female',
 'HH_COMP_DESC_Single Male',
 'HH_COMP_DESC_Unknown',
 'HH_COMP_DESC_HH_COMP_DESC_nan',
 'HOUSEHOLD_SIZE_DESC_1',
 'HOUSEHOLD_SIZE_DESC_2',
 'HOUSEHOLD_SIZE_DESC_3',
 'HOUSEHOLD_SIZE_DESC_4',
 'HOUSEHOLD_SIZE_DESC_5+',
 'HOUSEHOLD_SIZE_DESC_HOUSEHOLD_SIZE_DESC_nan',
 'KID_CATEGORY_DESC_1',
 'KID_CATEGORY_DESC_2',
 'KID_CATEGORY_DESC_3+',
 'KID_CATEGORY_DESC_None/Unknown',
 'KID_CATEGORY_DESC_KID_CATEGORY_DESC_nan']

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

import pandas as pd 

dum_df = df_clean.toPandas()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(dum_df)
y = dum_df['churn']


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

#Instead of use the autolog we can create the artifacts by ourselves

import mlflow
#mlflow.create_experiment("churn-dunnhumby")
mlflow.set_experiment("churn-dunnhumby")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

from mlflow.models import infer_signature
# Model tags for MLflow
model_tags = {
    "project_name": "powerbi-summit",
    "store_dept": "sales",
    "team": "stores-ml",
    "project_quarter": "Q1-2025"
}

with mlflow.start_run() as run:

     from sklearn.neighbors import KNeighborsClassifier
     from sklearn.model_selection import train_test_split
     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
     knn = KNeighborsClassifier()
     knn.fit(X_train, y_train)
     
     # Infer the model signature
     signature = infer_signature(X, knn.predict(X_test))
     #prediction
     pred = knn.predict(X_train)
     
     #get metrics
     score_training = knn.score(X_train, y_train)
     score_test =  knn.score(X_test, y_test)
     
     # We can register training metrics 
     mlflow.log_metrics({"score_training" : score_training , "score_test" : score_test})
     
     #Register the model
     mlflow.sklearn.log_model(knn , "modelo-churn-dunnhumby" , signature= signature)
     
     #Create the model to reutilize it
     mlflow.register_model("runs:/{}/modelo-churn-dunnhumby".format(run.info.run_id) , "demo-churn-dunhumby" , tags = model_tags)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

print(confusion_matrix(y_train, pred))
print(classification_report(y_train, pred))

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
