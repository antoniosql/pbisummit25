# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "feb712bf-1546-445c-bea4-665c82c12f2b",
# META       "default_lakehouse_name": "pbisummitlake",
# META       "default_lakehouse_workspace_id": "4a689fc0-cc6e-482f-b6cf-3026a575a7c1",
# META       "known_lakehouses": [
# META         {
# META           "id": "feb712bf-1546-445c-bea4-665c82c12f2b"
# META         }
# META       ]
# META     },
# META     "environment": {
# META       "environmentId": "28dbaf6b-8fc5-89a4-444a-db3b58a22f66",
# META       "workspaceId": "00000000-0000-0000-0000-000000000000"
# META     }
# META   }
# META }

# MARKDOWN ********************

# # Low code Automated ML 
# ## Introduction
# 
# This notebook is automatically generated by a low-code UI wizard based on the settings provided; feel free to adjust these settings to fine-tune the results to your preference.
# 
# The main steps in this notebook are:
# 
# 1. Load the Data
# 2. Featurization
# 3. Train an AutoML Trial with FLAML to Find the Best Model
# 4. Save the Final Machine Learning Model
# 5. Predicting with the Saved Model.
# 
# > [!IMPORTANT]
# > **Automated ML is currently supported on Fabric Runtimes 1.2+ or any Fabric environment with Spark 3.4+.**

# CELL ********************

%pip install scikit-learn==1.5.1


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

import logging
import warnings
 
logging.getLogger('synapse.ml').setLevel(logging.CRITICAL)
logging.getLogger('mlflow.utils').setLevel(logging.CRITICAL)
warnings.simplefilter('ignore', category=FutureWarning)
warnings.simplefilter('ignore', category=UserWarning)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Step 1: Load the Data

# MARKDOWN ********************

# - Reads raw data from the given path, transforming the data according to the selected model type. 


# CELL ********************

import re
df = spark.read.format("delta").load(
    "Tables/customertransactions"
).cache()
df = df.toDF(*(re.sub('[^A-Za-z0-9_]+', '_', c) for c in df.columns))  # Replace not supported characters in column name with underscore to avoid invalid character for model training and saving

target_col = re.sub('[^A-Za-z0-9_]+', '_', "churn")


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

display(df)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Step 2: Featurization
# 
# The next step is to prepare the data for training. This process involves casting the data types, handling missing values, etc. The data is then split into training and testing sets.


# CELL ********************

# Set Functions if needed for Featurization
from pyspark.sql.types import *

def filter_supported_columns(df, feature_cols):
    supported_types = (FloatType, DoubleType, ShortType, IntegerType, LongType)  # Types supported by VectorAssembler
    supported_cols = [col_name for col_name in feature_cols
                      if isinstance(df.schema[col_name].dataType, supported_types)]
   
    return supported_cols


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Import the necessary library for feature vectorization
from pyspark.ml.feature import VectorAssembler
from flaml.automl.spark.utils import to_pandas_on_spark


# Train-Test Separation
train_raw, test_raw = df.randomSplit([0.8, 0.2], seed=41)
# Fill NA values with 0 for the numeric columns
train_raw = train_raw.fillna(0)
test_raw = test_raw.fillna(0)

# Define the feature columns (excluding the target_name variable "churn")
feature_cols = [col for col in df.columns if col != target_col]
feature_cols = filter_supported_columns(df, feature_cols)

# Create a VectorAssembler to combine feature columns into a single 'features' column
featurizer = VectorAssembler(inputCols=feature_cols, outputCol="features", handleInvalid="keep")

# Transform the training and testing datasets using the VectorAssembler
train_data = featurizer.transform(train_raw)[target_col, "features"]
test_data = featurizer.transform(test_raw)[target_col, "features"]

# Transform to pandas on spark according to the selected models
df_train = to_pandas_on_spark(train_data)
df_test = to_pandas_on_spark(test_data)


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Step 3: Train an AutoML Trial with FLAML to Find the Best Model

# MARKDOWN ********************

# With your data in place, you can now define the model. You will also use MLfLow and Fabric Autologging to track the experiments.

# MARKDOWN ********************

# ### Set up MLflow experiment tracking
# 
# MLflow is an open source platform that is deeply integrated into the Data Science experience in Fabric and allows to easily track and compare the performance of different models and experiments without the need for manual tracking. For more information, see [Autologging in Microsoft Fabric](https://aka.ms/fabric-autologging).

# CELL ********************

# MLFlow Logging Related

import mlflow

mlflow.autolog(exclusive=False)
mlflow.set_experiment("experimentAutoML")


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# #### Configure the AutoML trial and settings
# 
# Import the required classes and modules from the FLAML package and instantiate AutoML, which automates the machine learning pipeline.

# CELL ********************

# Import the AutoML class from the FLAML package
from flaml import AutoML

# Define AutoML settings
settings = {
    "time_budget": 1800, # Total running time in seconds
    "task": "binary", 
    "log_file_name": "flaml_experiment.log",  # FLAML log file
    "eval_method": "cv",
    "n_splits": 3,
    "seed": 41 , # Random seed 
    "mlflow_exp_name": "experimentAutoML",  # MLflow experiment name
    "verbose": 1, 
    "featurization": "auto", 
}

# Create an AutoML instance
automl = AutoML(**settings)


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# #### Run the AutoML trial
# 
# Execute the AutoML trial, using a nested MLflow run to track the experiment within the existing MLflow run context. The trial is conducted on the processed dataset with the target variable ``churn``, and the defined settings are passed to the `fit` function for configuration.

# CELL ********************

with mlflow.start_run(nested=True, run_name="AutoMLModel"):
    automl.fit(
        dataframe=df_train, 
        label=target_col, 
    )

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Step 4: Save as the final machine learning model

# MARKDOWN ********************

# Upon completing the AutoML trial, you can now save the final, tuned model as an ML model in Fabric.

# CELL ********************

model_path = f"runs:/{automl.best_run_id}/model"

# Register the model to the MLflow registry
registered_model = mlflow.register_model(model_uri=model_path, name="AutoMLModel")

# Print the registered model's name and version
print(f"Model '{registered_model.name}' version {registered_model.version} registered successfully.")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Step 5: Predicting with the saved model.

# MARKDOWN ********************

# Microsoft Fabric allows users to operationalize machine learning models with a scalable function called `PREDICT`, which supports batch scoring (or batch inferencing) in any compute engine.
# 
# You can generate batch predictions directly from the Microsoft Fabric notebook or from a given model's item page. For more information on how to use `PREDICT`, see [Model scoring with PREDICT in Microsoft Fabric](https://aka.ms/fabric-predict).
# 
# 1. Load the model for batch scoring and generate the prediction results.

# CELL ********************

model_name = "AutoMLModel"
model = mlflow.spark.load_model(f"models:/{registered_model.name}/{registered_model.version}")


batch_predictions = model.transform(df_test)


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

display(batch_predictions)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# 2. Save the predictions to a table.

# CELL ********************

saved_name = "customertransactions_predictions_automl".replace(".", "_")
batch_predictions.write.mode("overwrite").format("delta").option("overwriteSchema", "true").save(f"Tables/{saved_name}")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
